{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5dee498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johnd\\anaconda3\\envs\\birdset\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\johnd\\anaconda3\\envs\\birdset\\lib\\site-packages\\torch_audiomentations\\utils\\io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "sampling: 100%|██████████| 48/48 [00:04<00:00, 10.59it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b3fcb92ae34db388c3cd8007bc4ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/41115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94714eb2ca7446e96e44dc5909b3f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10279 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee10bddc6eb042b09afb9c3bec705e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/16052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from birdset.datamodule import DatasetConfig\n",
    "from birdset.datamodule.birdset_datamodule import BirdSetDataModule\n",
    "from birdset.datamodule import LoadersConfig, LoaderConfig\n",
    "\n",
    "dm = BirdSetDataModule(\n",
    "    dataset=DatasetConfig(\n",
    "        data_dir=\"./datasets\",\n",
    "        hf_path=\"DBD-research-group/BirdSet\",\n",
    "        hf_name=\"POW\",\n",
    "        n_workers=21,\n",
    "        val_split=0.2,\n",
    "        task=\"multiclass\",\n",
    "        classlimit=500,\n",
    "        eventlimit=5,\n",
    "        sample_rate=32000,\n",
    "    ),\n",
    "    loaders=LoadersConfig(\n",
    "        train=LoaderConfig(batch_size=8, shuffle=True),\n",
    "        valid=LoaderConfig(batch_size=8, shuffle=False),\n",
    "        test=LoaderConfig(batch_size=8, shuffle=True),\n",
    "    ),\n",
    ")\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup(stage=\"fit\")\n",
    "\n",
    "train_loader = dm.train_dataset\n",
    "validation_loader = dm.val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dfeb348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "num_classes = dm.num_classes\n",
    "\n",
    "def data_generator(dataset, augment=False):\n",
    "    for sample in dataset:\n",
    "        # Get audio and label\n",
    "        audio = sample['input_values']\n",
    "        label = sample['labels']\n",
    "        \n",
    "        # Handle the shape - BirdSet returns shape (1, 32000) or (batch, 32000)\n",
    "        if isinstance(audio, np.ndarray):\n",
    "            audio = audio.flatten()  # Force to 1D\n",
    "        else:\n",
    "            audio = audio.numpy().flatten()  # Force to 1D\n",
    "        \n",
    "        # Ensure audio is exactly 1D with 32000 samples\n",
    "        if audio.shape[0] != 32000:\n",
    "            # Pad or trim to 32000\n",
    "            if audio.shape[0] < 32000:\n",
    "                audio = np.pad(audio, (0, 32000 - audio.shape[0]), mode='constant')\n",
    "            else:\n",
    "                audio = audio[:32000]\n",
    "        \n",
    "        # Extract label and convert to int\n",
    "        if isinstance(label, np.ndarray):\n",
    "            label = int(label.item() if label.size == 1 else label.flat[0])\n",
    "        else:\n",
    "            label = int(label.numpy().item() if label.numpy().size == 1 else label.numpy().flat[0])\n",
    "        \n",
    "        audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "        audio_tensor = tf.reshape(audio_tensor, [-1])\n",
    "        \n",
    "        # Compute STFT\n",
    "        stft = tf.signal.stft(audio_tensor, frame_length=2048, frame_step=512, fft_length=2048)\n",
    "        spectrogram = tf.abs(stft)\n",
    "        \n",
    "        # Convert to mel scale (80 mel bins is standard)\n",
    "        num_spectrogram_bins = spectrogram.shape[-1]\n",
    "        lower_edge_hertz, upper_edge_hertz = 80.0, 7600.0\n",
    "        num_mel_bins = 128\n",
    "        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins, num_spectrogram_bins, 32000, lower_edge_hertz, upper_edge_hertz\n",
    "        )\n",
    "        mel_spectrogram = tf.tensordot(spectrogram, linear_to_mel_weight_matrix, 1)\n",
    "        mel_spectrogram.set_shape(spectrogram.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "        \n",
    "        # Convert to log scale (dB)\n",
    "        log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        spectrogram = log_mel_spectrogram.numpy()\n",
    "        \n",
    "        # Transpose to (time, mel_bins)\n",
    "        spectrogram = spectrogram.T\n",
    "        \n",
    "        # Add channel dimension\n",
    "        spectrogram = spectrogram[..., np.newaxis]\n",
    "        \n",
    "        # Resize to 224x224x1\n",
    "        spectrogram_resized = tf.image.resize(spectrogram, [224, 224], method='bilinear').numpy()\n",
    "        \n",
    "        # Repeat to 3 channels\n",
    "        spectrogram_3ch = np.repeat(spectrogram_resized, 3, axis=-1)\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        spec_min = spectrogram_3ch.min()\n",
    "        spec_max = spectrogram_3ch.max()\n",
    "        spectrogram_3ch = (spectrogram_3ch - spec_min) / (spec_max - spec_min + 1e-8)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        label_onehot = tf.one_hot(label, num_classes).numpy()\n",
    "\n",
    "        if augment:\n",
    "            # Random time/frequency masking (SpecAugment)\n",
    "            spectrogram_3ch = tf.image.random_brightness(spectrogram_3ch, 0.2).numpy()\n",
    "            \n",
    "            # Random horizontal flip (time reversal)\n",
    "            if np.random.random() > 0.5:\n",
    "                spectrogram_3ch = np.flip(spectrogram_3ch, axis=1)\n",
    "        \n",
    "        yield spectrogram_3ch.astype(np.float32), label_onehot.astype(np.float32)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(dm.train_dataset, augment=False),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(num_classes,), dtype=tf.float32),\n",
    "    )\n",
    ").batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(dm.val_dataset),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(num_classes,), dtype=tf.float32),\n",
    "    )\n",
    ").batch(8).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c6e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to use Tensorflow Keras for the model\n",
    "import keras as ks\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "num_classes = dm.num_classes\n",
    "\n",
    "# Copy efficientnetb0 architecture\n",
    "model = ks.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "x = model.output\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(num_classes, activation='softmax')(x)\n",
    "model = ks.models.Model(inputs=model.input, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a814c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnd\\BirdSet\\birdset\\datamodule\\components\\transforms.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5140/5140 [==============================] - 3736s 726ms/step - loss: 3.5826 - accuracy: 0.1110 - val_loss: 6.1050 - val_accuracy: 0.0979 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "5140/5140 [==============================] - 3658s 712ms/step - loss: 3.4590 - accuracy: 0.1230 - val_loss: 3.6329 - val_accuracy: 0.1166 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "5140/5140 [==============================] - 3709s 722ms/step - loss: 3.3831 - accuracy: 0.1408 - val_loss: 3.3785 - val_accuracy: 0.1437 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "5140/5140 [==============================] - 3682s 716ms/step - loss: 3.3108 - accuracy: 0.1526 - val_loss: 3.3914 - val_accuracy: 0.1390 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "5140/5140 [==============================] - 3716s 723ms/step - loss: 3.2548 - accuracy: 0.1644 - val_loss: 3.3609 - val_accuracy: 0.1518 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "5140/5140 [==============================] - 3705s 721ms/step - loss: 3.1968 - accuracy: 0.1759 - val_loss: 3.2272 - val_accuracy: 0.1776 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "5140/5140 [==============================] - 3734s 726ms/step - loss: 3.1462 - accuracy: 0.1850 - val_loss: 3.1885 - val_accuracy: 0.1821 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "5140/5140 [==============================] - 3774s 734ms/step - loss: 3.0964 - accuracy: 0.1988 - val_loss: 3.4775 - val_accuracy: 0.1349 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "5140/5140 [==============================] - 3741s 728ms/step - loss: 3.0436 - accuracy: 0.2106 - val_loss: 3.1189 - val_accuracy: 0.1998 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "5140/5140 [==============================] - 3697s 719ms/step - loss: 2.9953 - accuracy: 0.2191 - val_loss: 3.2532 - val_accuracy: 0.1819 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca11425f90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# let's train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a429976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5140/5140 [==============================] - 3714s 722ms/step - loss: 1.9825 - accuracy: 0.4587 - val_loss: 3.1959 - val_accuracy: 0.2586 - lr: 5.0000e-04\n",
      "Epoch 2/10\n",
      "5140/5140 [==============================] - 3835s 746ms/step - loss: 1.8749 - accuracy: 0.4886 - val_loss: 3.3088 - val_accuracy: 0.2563 - lr: 5.0000e-04\n",
      "Epoch 3/10\n",
      "5140/5140 [==============================] - 3988s 776ms/step - loss: 1.7830 - accuracy: 0.5128 - val_loss: 3.2372 - val_accuracy: 0.2445 - lr: 5.0000e-04\n",
      "Epoch 4/10\n",
      "5140/5140 [==============================] - 3804s 740ms/step - loss: 1.6789 - accuracy: 0.5381 - val_loss: 3.9953 - val_accuracy: 0.2093 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "5140/5140 [==============================] - 3841s 747ms/step - loss: 1.4983 - accuracy: 0.5912 - val_loss: 3.2759 - val_accuracy: 0.2731 - lr: 2.5000e-04\n",
      "Epoch 6/10\n",
      "5140/5140 [==============================] - 4042s 786ms/step - loss: 1.3960 - accuracy: 0.6193 - val_loss: 3.1806 - val_accuracy: 0.2955 - lr: 2.5000e-04\n",
      "Epoch 7/10\n",
      "5140/5140 [==============================] - 3902s 759ms/step - loss: 1.3208 - accuracy: 0.6390 - val_loss: 3.3442 - val_accuracy: 0.2856 - lr: 2.5000e-04\n",
      "Epoch 8/10\n",
      "5140/5140 [==============================] - 3949s 768ms/step - loss: 1.2467 - accuracy: 0.6561 - val_loss: 3.3269 - val_accuracy: 0.2944 - lr: 2.5000e-04\n",
      "Epoch 9/10\n",
      "5140/5140 [==============================] - 3826s 744ms/step - loss: 1.1705 - accuracy: 0.6787 - val_loss: 3.4088 - val_accuracy: 0.2866 - lr: 2.5000e-04\n",
      "Epoch 10/10\n",
      "5140/5140 [==============================] - 3827s 744ms/step - loss: 1.0681 - accuracy: 0.7068 - val_loss: 3.4021 - val_accuracy: 0.2759 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca80a39390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
